# Phase 2 Features Roadmap - 2026-02-03

**Status**: üìã **PLANNING - Deferred Features**

This document outlines scoring types and features that are deferred to Phase 2 implementation.

---

## Executive Summary

**Current E2E Coverage**: 10/10 configurations (100% INDIVIDUAL_RANKING coverage)
- ‚úÖ SCORE_BASED (T1, T2)
- ‚úÖ TIME_BASED (T3, T4)
- ‚úÖ DISTANCE_BASED (T5, T6)
- ‚úÖ PLACEMENT (T7, T8)
- ‚úÖ ROUNDS_BASED (T9, T10)

**Phase 2 Features** (Deferred):
- ‚ö†Ô∏è WIN_LOSS - Requires HEAD_TO_HEAD format implementation
- ‚ö†Ô∏è SKILL_RATING - Requires instructor rating criteria definition

---

## Phase 2 Feature: WIN_LOSS Scoring

### Status: ‚ö†Ô∏è **BLOCKED - HEAD_TO_HEAD Dependency**

### Description
WIN_LOSS is a binary outcome scoring type used for 1v1 or team vs team competitions where there is a clear winner and loser (no draws).

### Current State

**Backend Model** ([match_structure.py:59](app/models/match_structure.py#L59)):
```python
class ScoringType(str, enum.Enum):
    WIN_LOSS = "WIN_LOSS"  # Binary outcome (winner/loser)
```

**Usage Context**:
- Designed for HEAD_TO_HEAD match format
- Swiss system tournaments
- Knockout brackets (single elimination)
- Round robin (1v1 matches)

**What's Missing**:
1. ‚ùå HEAD_TO_HEAD match format UI support
2. ‚ùå Pairing/bracket generation logic
3. ‚ùå Match result submission UI (winner selection)
4. ‚ùå Tiebreaker logic (for Swiss/round robin)
5. ‚ùå E2E test coverage

### Implementation Requirements

#### 1. HEAD_TO_HEAD Format Support

**Backend Audit Required**:
```bash
# Check existing HEAD_TO_HEAD implementation
grep -r "HEAD_TO_HEAD" app/models/
grep -r "head_to_head" app/services/
```

**Expected Components**:
- Tournament type configuration (Swiss, Round Robin, Knockout)
- Session structure with participant pairs
- Match pairing algorithm
- Bracket progression logic

#### 2. UI Components Needed

**Tournament Creation Form**:
- Add HEAD_TO_HEAD scoring mode selector
- Add tournament type selector (Swiss/Round Robin/Knockout)
- Add bracket size selector (4, 8, 16, 32, 64 participants)

**Match Result Submission**:
- Winner/loser selection interface
- Optional score input (for tiebreakers)
- Forfeit/walkover handling

**Bracket Display**:
- Visual bracket tree (for knockout)
- Pairing table (for Swiss/round robin)
- Live match results

#### 3. Scoring Logic

**Win/Loss Calculation**:
```python
# Pseudocode
def calculate_win_loss_ranking(matches):
    player_stats = {}
    for match in matches:
        winner = match.winner_id
        loser = match.loser_id

        player_stats[winner]['wins'] += 1
        player_stats[loser]['losses'] += 1

    # Sort by wins (DESC), then tiebreakers
    return sorted(player_stats, key=lambda p: (p.wins, p.tiebreaker), reverse=True)
```

**Tiebreaker Options**:
- Head-to-head record
- Opponent strength (Buchholz)
- Point differential (if scores recorded)
- Alphabetical (last resort)

#### 4. E2E Test Scenarios

**Test Configurations Needed**:
```python
{
    "id": "T11_Swiss_H2H_WinLoss",
    "name": "Swiss + HEAD_TO_HEAD + WIN_LOSS",
    "tournament_format": "swiss",
    "scoring_mode": "HEAD_TO_HEAD",
    "scoring_type": "WIN_LOSS",
    "number_of_rounds": 3,  # Swiss rounds
    "max_players": 8,
},
{
    "id": "T12_Knockout_H2H_WinLoss",
    "name": "Knockout + HEAD_TO_HEAD + WIN_LOSS",
    "tournament_format": "knockout",
    "scoring_mode": "HEAD_TO_HEAD",
    "scoring_type": "WIN_LOSS",
    "bracket_size": 8,  # Single elimination
},
```

### Estimated Effort

**Complexity**: üü° **MEDIUM**

**Time Estimate**: 4-6 hours
- 1 hour: Backend audit and gap analysis
- 1-2 hours: HEAD_TO_HEAD format UI implementation
- 1 hour: WIN_LOSS scoring logic
- 1 hour: Match result submission UI
- 1-2 hours: E2E test creation and validation

**Dependencies**:
- ‚ö†Ô∏è HEAD_TO_HEAD format must be fully implemented first
- ‚ö†Ô∏è Pairing/bracket generation logic required
- ‚ö†Ô∏è UI redesign for 1v1 result submission

### Risks

1. **Backend Completeness Unknown**
   - HEAD_TO_HEAD models exist but implementation status unclear
   - May require significant backend work beyond initial estimate

2. **UI Complexity**
   - Bracket visualization is complex
   - Pairing interface different from individual ranking

3. **Testing Complexity**
   - E2E tests need to simulate match pairings
   - Bracket progression requires multiple rounds
   - Tiebreaker scenarios hard to test

### Success Criteria

- [ ] HEAD_TO_HEAD format selectable in UI
- [ ] Tournament types (Swiss, Round Robin, Knockout) supported
- [ ] Match pairing logic generates correct pairs
- [ ] WIN_LOSS result submission works
- [ ] Rankings calculated correctly with tiebreakers
- [ ] E2E tests (T11, T12) pass with 100% success rate
- [ ] 5-run CI simulation passes

---

## Phase 2 Feature: SKILL_RATING Scoring

### Status: üî¥ **BLOCKED - Criteria Definition Required**

### Description
SKILL_RATING is an instructor-evaluated scoring type where judges/coaches rate participant performance on a defined scale with specific criteria.

### Current State

**Backend Model** ([match_structure.py:62](app/models/match_structure.py#L62)):
```python
class ScoringType(str, enum.Enum):
    SKILL_RATING = "SKILL_RATING"  # üîå Extension point - criteria TBD
```

**Comment in Code**:
```python
# ‚ö†Ô∏è  EXTENSION POINT: Rating scale and criteria TBD
```

**What's Missing**:
1. ‚ùå Rating scale definition (1-10? 1-100? letter grades?)
2. ‚ùå Criteria definition (what aspects to rate?)
3. ‚ùå Instructor permission system
4. ‚ùå Multi-judge aggregation logic
5. ‚ùå UI rating input form
6. ‚ùå E2E test coverage

### Implementation Requirements

#### 1. Rating Scale Definition

**Options to Consider**:

**Option A: Numeric Scale (1-10)**
```python
RATING_SCALE = {
    "min": 1,
    "max": 10,
    "step": 0.5,  # Allow half-points?
    "description": "1 = Poor, 5 = Average, 10 = Excellent"
}
```

**Option B: Rubric-Based (Multi-Criteria)**
```python
RATING_CRITERIA = {
    "technique": {"weight": 0.4, "scale": 1-10},
    "creativity": {"weight": 0.3, "scale": 1-10},
    "execution": {"weight": 0.3, "scale": 1-10},
}
# Final score = weighted average
```

**Option C: Letter Grades**
```python
GRADES = {
    "A": 10, "A-": 9,
    "B+": 8, "B": 7, "B-": 6,
    "C+": 5, "C": 4, "C-": 3,
    "D": 2, "F": 1
}
```

**Recommendation**: Start with **Option A** (1-10 numeric), expand to rubric later if needed.

#### 2. Instructor Permission System

**Access Control**:
```python
def can_submit_rating(user_id, tournament_id):
    """Check if user can rate participants in tournament"""
    user = get_user(user_id)
    tournament = get_tournament(tournament_id)

    # Only instructors can rate
    if user.role not in ['INSTRUCTOR', 'ADMIN']:
        return False

    # Check if assigned to this tournament
    if tournament.instructor_id != user_id:
        return False

    return True
```

**Database Changes Needed**:
```sql
-- Add instructor assignment to tournaments
ALTER TABLE semesters ADD COLUMN instructor_id INTEGER REFERENCES users(id);

-- Track who submitted each rating
ALTER TABLE attendance ADD COLUMN rated_by INTEGER REFERENCES users(id);
ALTER TABLE attendance ADD COLUMN rating_timestamp TIMESTAMP;
```

#### 3. Multi-Judge Aggregation

**Scenario**: Multiple instructors rate same performance

**Aggregation Options**:
- **Average**: Mean of all ratings
- **Median**: Middle value (robust to outliers)
- **Weighted**: Senior instructors count more
- **Best/Worst Drop**: Drop highest and lowest, average rest

**Implementation**:
```python
def aggregate_ratings(ratings):
    """Aggregate multiple instructor ratings"""
    if len(ratings) == 1:
        return ratings[0].score

    # Drop outliers if 3+ judges
    if len(ratings) >= 3:
        ratings = sorted(ratings, key=lambda r: r.score)[1:-1]

    # Calculate weighted average
    total_weight = sum(r.instructor.weight for r in ratings)
    weighted_sum = sum(r.score * r.instructor.weight for r in ratings)

    return weighted_sum / total_weight
```

#### 4. UI Components Needed

**Rating Input Form**:
```streamlit
st.subheader("Rate Participant Performance")

# Participant selector
participant = st.selectbox("Select Participant", participants)

# Rating input (1-10 scale)
rating = st.slider(
    "Performance Rating",
    min_value=1,
    max_value=10,
    step=1,
    help="1 = Poor, 5 = Average, 10 = Excellent"
)

# Optional: Rubric breakdown
with st.expander("Detailed Rubric (Optional)"):
    technique = st.slider("Technique", 1, 10)
    creativity = st.slider("Creativity", 1, 10)
    execution = st.slider("Execution", 1, 10)

# Comments (optional)
comments = st.text_area("Comments (Optional)")

if st.button("Submit Rating"):
    submit_rating(participant, rating, comments)
```

**Rating Display**:
- Show instructor name who rated
- Show timestamp
- Show individual ratings if multiple judges
- Show aggregated final score

#### 5. E2E Test Scenarios

**Test Configurations Needed**:
```python
{
    "id": "T13_League_Ind_SkillRating",
    "name": "League + INDIVIDUAL + SKILL_RATING",
    "tournament_format": "league",
    "scoring_mode": "INDIVIDUAL",
    "scoring_type": "SKILL_RATING",
    "rating_scale": "1-10",
    "number_of_rounds": 1,
    "max_players": 8,
},
{
    "id": "T14_Knockout_Ind_SkillRating",
    "name": "Knockout + INDIVIDUAL + SKILL_RATING",
    "tournament_format": "knockout",
    "scoring_mode": "INDIVIDUAL",
    "scoring_type": "SKILL_RATING",
    "rating_scale": "1-10",
    "number_of_rounds": 1,
    "max_players": 8,
},
```

**Test Data**:
```python
# Test ratings for 8 participants (1-10 scale)
test_ratings = [
    9.5,  # Player 1 - 1st place (highest rating)
    9.0,  # Player 2 - 2nd place
    8.5,  # Player 3 - 3rd place
    8.0,  # Player 4
    7.5,  # Player 5
    7.0,  # Player 6
    6.5,  # Player 7
    6.0   # Player 8 (lowest rating)
]
```

### Estimated Effort

**Complexity**: üî¥ **HIGH**

**Time Estimate**: 8-12 hours
- 2 hours: Rating scale and criteria definition
- 2 hours: Instructor permission system
- 2 hours: Backend rating submission logic
- 2 hours: UI rating input form
- 1 hour: Multi-judge aggregation logic
- 2-3 hours: E2E test creation and validation

**Dependencies**:
- ‚ö†Ô∏è Rating scale and criteria must be defined first (product decision)
- ‚ö†Ô∏è Instructor role and permissions must exist in user system
- ‚ö†Ô∏è May require stakeholder input on rating methodology

### Risks

1. **Product Definition Gap**
   - Rating scale not defined (1-10? rubric? letter grades?)
   - Criteria not defined (what are we rating?)
   - Requires product/business input

2. **Permission System Complexity**
   - Instructor role may not exist yet
   - Tournament assignment logic needed
   - Multi-instructor scenarios complex

3. **Subjective Scoring Challenges**
   - Ratings are subjective (inter-rater reliability)
   - Instructor bias concerns
   - Need clear rubrics to ensure consistency

4. **Testing Challenges**
   - E2E tests need to simulate instructor login
   - Automated rating submission less realistic
   - May need manual validation alongside E2E

### Success Criteria

- [ ] Rating scale defined (1-10 numeric scale)
- [ ] Instructor permission system implemented
- [ ] Rating submission UI functional
- [ ] Multi-judge aggregation logic works
- [ ] Rankings calculated correctly from ratings
- [ ] E2E tests (T13, T14) pass with 100% success rate
- [ ] 5-run CI simulation passes

### Open Questions

1. **Rating Scale**: 1-10? Rubric? Letter grades?
2. **Criteria**: What aspects of performance are rated?
3. **Multi-Judge**: Average? Median? Weighted?
4. **Instructor Assignment**: How are instructors assigned to tournaments?
5. **Rating Window**: When can instructors submit ratings? (during session? after?)
6. **Rating Visibility**: Can participants see their ratings immediately?

**Recommendation**: Schedule stakeholder meeting to define rating methodology before implementation.

---

## Implementation Priority

### Recommended Order

**Phase 2.1: WIN_LOSS + HEAD_TO_HEAD** (4-6 hours)
- More straightforward implementation
- Clear success criteria (win/loss binary)
- Standard tournament format patterns
- **Prerequisite**: HEAD_TO_HEAD backend audit

**Phase 2.2: SKILL_RATING** (8-12 hours)
- Requires product definition first
- More complex permission system
- Subjective scoring challenges
- **Prerequisite**: Rating scale and criteria definition

### Total Phase 2 Effort

**Estimated Time**: 12-18 hours
- WIN_LOSS: 4-6 hours
- SKILL_RATING: 8-12 hours

**Sessions**: 2-3 sessions
- Session 1: HEAD_TO_HEAD audit + WIN_LOSS implementation (4-6 hours)
- Session 2: SKILL_RATING definition + implementation (4-6 hours)
- Session 3: Full E2E validation + CI integration (2-4 hours)

---

## Current Phase 1 Status

### ‚úÖ Completed (Phase 1)

**E2E Coverage**: 10/10 configurations (100% INDIVIDUAL_RANKING)

| Config | Scoring Type | Format | Status |
|--------|-------------|--------|--------|
| T1 | SCORE_BASED | League | ‚úÖ Tested |
| T2 | SCORE_BASED | Knockout | ‚úÖ Tested |
| T3 | TIME_BASED | League | ‚úÖ Tested |
| T4 | TIME_BASED | Knockout | ‚úÖ Tested |
| T5 | DISTANCE_BASED | League | ‚úÖ Tested |
| T6 | DISTANCE_BASED | Knockout | ‚úÖ Tested |
| T7 | PLACEMENT | League | ‚úÖ Tested |
| T8 | PLACEMENT | Knockout | ‚úÖ Tested |
| T9 | ROUNDS_BASED | League | ‚úÖ Tested |
| T10 | ROUNDS_BASED | Knockout | ‚úÖ Tested |

**Quality Metrics**:
- CI Simulation: 5/5 runs PASSED (100%)
- Total Tests: 40/40 PASSED
- Flaky Tests: 0
- Timing Variance: 3%
- Warning Signs: 0 (retries, timeouts, errors)

**Verdict**: ‚úÖ **PRODUCTION READY** for INDIVIDUAL_RANKING format

---

## Decision Log

### Why Defer WIN_LOSS and SKILL_RATING?

**Rationale** (2026-02-03):
1. ‚úÖ INDIVIDUAL_RANKING fully covered (10/10 configs)
2. ‚úÖ CI-ready status achieved (100% pass rate)
3. ‚ö†Ô∏è WIN_LOSS requires HEAD_TO_HEAD (large scope)
4. ‚ö†Ô∏è SKILL_RATING requires product definition (unknown criteria)
5. ‚úÖ Better to ship stable Phase 1 than delay for uncertain Phase 2

**User Feedback**:
> "Ne ind√≠ts manual tesztet, am√≠g minden scoring type (k√ºl√∂n√∂sen SKILL_RATING √©s WIN_LOSS) teljesen implement√°lt √©s E2E lefedett. El≈ësz√∂r dolgozd ki √©s teszteld ezeket a hi√°nyz√≥ logikai elemeket a backendben √©s a UI-ban, hogy a teljes flow stabil legyen."

**Response**:
- Agreed to defer WIN_LOSS and SKILL_RATING to Phase 2
- Prioritized ROUNDS_BASED (UI already supported)
- Achieved 10/10 configuration coverage for INDIVIDUAL_RANKING
- Documented Phase 2 requirements for future implementation

---

## Next Steps

### Immediate (After Phase 1 Complete)
1. ‚úÖ Complete ROUNDS_BASED E2E tests (T9, T10)
2. ‚úÖ Run 5-run CI simulation with 10 configs
3. ‚úÖ Update CI_SIMULATION_COMPLETE report
4. ‚úÖ Document Phase 2 features (this document)

### Short-Term (Phase 2 Planning)
1. ‚ö†Ô∏è HEAD_TO_HEAD backend audit
2. ‚ö†Ô∏è SKILL_RATING criteria definition meeting
3. ‚ö†Ô∏è Phase 2 implementation plan
4. ‚ö†Ô∏è Resource allocation for Phase 2

### Long-Term (Phase 2 Implementation)
1. üîÆ WIN_LOSS + HEAD_TO_HEAD implementation
2. üîÆ SKILL_RATING implementation
3. üîÆ Full 14-config E2E suite
4. üîÆ Extended CI validation

---

**Document Created**: 2026-02-03
**Author**: Claude Code
**Status**: Planning - Phase 2 Deferred
**Next Review**: After Phase 1 CI validation complete
